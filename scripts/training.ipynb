{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EchoDataset' from 'dataset' (c:\\Users\\cm_cj\\Desktop\\Tec\\AI\\EchoNetDynamic\\scripts\\dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cm_cj\\Desktop\\Tec\\AI\\EchoNetDynamic\\scripts\\training.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     load_checkpoint,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     save_checkpoint,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     get_loaders,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     check_accuracy,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     save_predictions_as_imgs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cm_cj/Desktop/Tec/AI/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cm_cj\\Desktop\\Tec\\AI\\EchoNetDynamic\\scripts\\utils.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m EchoDataset\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'EchoDataset' from 'dataset' (c:\\Users\\cm_cj\\Desktop\\Tec\\AI\\EchoNetDynamic\\scripts\\dataset.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import UNET\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from utils import (\n",
    "    load_checkpoint,\n",
    "    save_checkpoint,\n",
    "    get_loaders,\n",
    "    check_accuracy,\n",
    "    save_predictions_as_imgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 32494/3587584 with acc 0.91\n",
      "Dice score: 1.9061719179153442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.91it/s, loss=-4.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n",
      "y torch.Size([1, 112, 112])\n",
      "preds torch.Size([1, 1, 112, 112])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\musel\\EchoNetDynamic\\scripts\\training.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m         save_predictions_as_imgs(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m             val_loader, model, folder\u001b[39m=\u001b[39mpath, device\u001b[39m=\u001b[39mDEVICE\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m         )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\musel\\EchoNetDynamic\\scripts\\training.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../saved_images/epoch_\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# print some examples to a folder\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m save_predictions_as_imgs(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     val_loader, model, folder\u001b[39m=\u001b[39;49mpath, device\u001b[39m=\u001b[39;49mDEVICE\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/musel/EchoNetDynamic/scripts/training.ipynb#W0sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\musel\\EchoNetDynamic\\scripts\\utils.py:107\u001b[0m, in \u001b[0;36msave_predictions_as_imgs\u001b[1;34m(loader, model, folder, device)\u001b[0m\n\u001b[0;32m    105\u001b[0m     plt\u001b[39m.\u001b[39mimshow(y\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze(),cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m     plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m     plt\u001b[39m.\u001b[39;49msavefig(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mfolder\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39mtest.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    109\u001b[0m     \u001b[39m#torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/{idx}.jpg\")\u001b[39;00m\n\u001b[0;32m    114\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\pyplot.py:1119\u001b[0m, in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1116\u001b[0m fig \u001b[39m=\u001b[39m gcf()\n\u001b[0;32m   1117\u001b[0m \u001b[39m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[39m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[1;32m-> 1119\u001b[0m res \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39msavefig(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mdraw_idle()  \u001b[39m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\figure.py:3390\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3388\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes:\n\u001b[0;32m   3389\u001b[0m         _recursively_make_axes_transparent(stack, ax)\n\u001b[1;32m-> 3390\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mprint_figure(fname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\backend_bases.py:2187\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2184\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2185\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2186\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[1;32m-> 2187\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[0;32m   2188\u001b[0m             filename,\n\u001b[0;32m   2189\u001b[0m             facecolor\u001b[39m=\u001b[39mfacecolor,\n\u001b[0;32m   2190\u001b[0m             edgecolor\u001b[39m=\u001b[39medgecolor,\n\u001b[0;32m   2191\u001b[0m             orientation\u001b[39m=\u001b[39morientation,\n\u001b[0;32m   2192\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2193\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2194\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   2195\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\backend_bases.py:2043\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2041\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m   2042\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[1;32m-> 2043\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2044\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m skip}))\n\u001b[0;32m   2045\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2046\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:514\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_jpg\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_jpg\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39m, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    510\u001b[0m     \u001b[39m# savefig() has already applied savefig.facecolor; we now set it to\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39m# white to make imsave() blend semi-transparent figures against an\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[39m# assumed white background.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     \u001b[39mwith\u001b[39;00m mpl\u001b[39m.\u001b[39mrc_context({\u001b[39m\"\u001b[39m\u001b[39msavefig.facecolor\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m\"\u001b[39m}):\n\u001b[1;32m--> 514\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mjpeg\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:445\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_print_pil\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    441\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     FigureCanvasAgg\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    446\u001b[0m     mpl\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave(\n\u001b[0;32m    447\u001b[0m         filename_or_obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_rgba(), \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mfmt, origin\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    448\u001b[0m         dpi\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mdpi, metadata\u001b[39m=\u001b[39mmetadata, pil_kwargs\u001b[39m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:388\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39mwith\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[0;32m    387\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[0;32m    389\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[0;32m     97\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\figure.py:3154\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3151\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3154\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3155\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3157\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[0;32m   3158\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 132\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py:3070\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3067\u001b[0m \u001b[39mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3068\u001b[0m     _draw_rasterized(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[1;32m-> 3070\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3071\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3073\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   3074\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 132\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\image.py:649\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[0;32m    648\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[0;32m    650\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[0;32m    651\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\image.py:939\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[1;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[0;32m    936\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[0;32m    937\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[0;32m    938\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[1;32m--> 939\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[0;32m    940\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\image.py:504\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[1;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[0;32m    502\u001b[0m vrange \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m offset\n\u001b[0;32m    503\u001b[0m \u001b[39m# resample the input data to the correct resolution and shape\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m A_resampled \u001b[39m=\u001b[39m _resample(\u001b[39mself\u001b[39;49m, A_scaled, out_shape, t)\n\u001b[0;32m    505\u001b[0m \u001b[39mdel\u001b[39;00m A_scaled  \u001b[39m# Make sure we don't use A_scaled anymore!\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# Un-scale the resampled data to approximately the original\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[39m# range. Things that interpolated to outside the original range\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39m# will still be outside, but possibly clipped in the case of\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m# higher order interpolation + drastically changing data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\musel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\image.py:208\u001b[0m, in \u001b[0;36m_resample\u001b[1;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mif\u001b[39;00m resample \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     resample \u001b[39m=\u001b[39m image_obj\u001b[39m.\u001b[39mget_resample()\n\u001b[1;32m--> 208\u001b[0m _image\u001b[39m.\u001b[39;49mresample(data, out, transform,\n\u001b[0;32m    209\u001b[0m                 _interpd_[interpolation],\n\u001b[0;32m    210\u001b[0m                 resample,\n\u001b[0;32m    211\u001b[0m                 alpha,\n\u001b[0;32m    212\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filternorm(),\n\u001b[0;32m    213\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filterrad())\n\u001b[0;32m    214\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 12\n",
    "NUM_WORKERS = 8\n",
    "IMAGE_HEIGHT = 112\n",
    "IMAGE_WIDTH = 112\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # path_prueba = f\"../prueba_images/\"\n",
    "\n",
    "        # torchvision.utils.save_image(targets, f\"{path_prueba}/{batch_idx}.jpg\")\n",
    "\n",
    "        # plt.imshow(targets.cpu().squeeze(),cmap=\"gray\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            # A.Rotate(limit=35, p=1.0),\n",
    "            # A.HorizontalFlip(p=0.5),\n",
    "            # A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    val_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
    "\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        # save_checkpoint(checkpoint)\n",
    "\n",
    "        # check accuracy\n",
    "        # check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "        path = f\"../saved_images/epoch_{epoch}\"\n",
    "\n",
    "        # print some examples to a folder\n",
    "        save_predictions_as_imgs(\n",
    "            val_loader, model, folder=path, device=DEVICE\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 112, 112]) torch.Size([112, 112])\n",
      "Image shape: torch.Size([3, 112, 112])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 8\n",
    "IMAGE_HEIGHT = 112\n",
    "IMAGE_WIDTH = 112\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"../data/train_images/\"\n",
    "TRAIN_MASK_DIR = \"../data/train_masks/\"\n",
    "VAL_IMG_DIR = \"../data/val_images/\"\n",
    "VAL_MASK_DIR = \"../data/val_masks/\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            # A.Rotate(limit=35, p=1.0),\n",
    "            # A.HorizontalFlip(p=0.5),\n",
    "            # A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    val_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    from dataset import EchoDataset\n",
    "\n",
    "    train_ds = EchoDataset(\n",
    "        image_dir=TRAIN_IMG_DIR,\n",
    "        mask_dir=TRAIN_MASK_DIR,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    image, label = train_ds[0]\n",
    "    print(image.shape, label.shape)\n",
    "\n",
    "    image, label = train_ds[0]\n",
    "    print(f\"Image shape: {image.shape}\")\n",
    "    # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "    plt.imshow(label, cmap=\"gray\")\n",
    "    # plt.title(label);\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
